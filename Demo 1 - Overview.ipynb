{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested Sampling with `dynesty`: The Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple interactive demo that briefly goes over nested sampling and some of the features available in `dynesty`. See the [documentation](dynesty.readthedocs.io) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's set up some environmental dependencies. These just make the numerics easier and adjust some of the plotting defaults to make things more legible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.19.5\n"
     ]
    }
   ],
   "source": [
    "# system functions that are always useful to have\n",
    "import time, sys, os\n",
    "\n",
    "# basic numeric setup\n",
    "import numpy as np\n",
    "print(np.version.version)\n",
    "from numpy import linalg\n",
    "\n",
    "# inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# plotting\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# seed the random number generator\n",
    "rstate = np.random.default_rng(5647)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration on 3-D Correlated Multivariate Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a sense for how `dynesty` works using a strongly correlated 3-D **multivariate Normal** distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dynesty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 3  # number of dimensions\n",
    "C = np.identity(ndim)  # set covariance to identity matrix\n",
    "C[C==0] = 0.95  # set off-diagonal terms (strongly correlated)\n",
    "Cinv = linalg.inv(C)  # precision matrix\n",
    "lnorm = -0.5 * (np.log(2 * np.pi) * ndim + np.log(linalg.det(C)))  # ln(normalization)\n",
    "\n",
    "# 3-D correlated multivariate normal log-likelihood\n",
    "def loglikelihood(x):\n",
    "    \"\"\"Multivariate normal log-likelihood.\"\"\"\n",
    "    return -0.5 * np.dot(x, np.dot(Cinv, x)) + lnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the documentation, `dynesty` samples over the unit cube. We'll define our prior (via `prior_transform`) to be uniform in each dimension from -10 to 10 and 0 everywhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior transform\n",
    "def prior_transform(u):\n",
    "    \"\"\"Transforms our unit cube samples `u` to a flat prior between -10. and 10. in each variable.\"\"\"\n",
    "    return 10. * (2. * u - 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, `dynesty` can also incorporate gradients when sampling, provided they are properly defined *with respect to the unit cube*. An example is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient of log-likelihood *with respect to u*\n",
    "# -> d(lnl)/du = d(lnl)/dv * dv/du\n",
    "# dv/du = 1. / prior(v)\n",
    "def gradient(x):\n",
    "    \"\"\"Multivariate normal log-likelihood gradient.\"\"\"\n",
    "    dlnl_dv = -np.dot(Cinv, x)  # standard gradient\n",
    "    jac = np.diag(np.full_like(x, 20.))  # Jacobian\n",
    "    return np.dot(jac, dlnl_dv)  # transformed gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling the Target Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now initialize our nested sampler. We'll use $K=1500$ live points (rather than the default $K=500$) along with `dynesty`'s default bounding/sampling modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our nested sampler\n",
    "sampler = dynesty.NestedSampler(loglikelihood, prior_transform, ndim, nlive=1500,rstate=rstate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Internally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now begin sampling from our target distribution. `NestedSampler` is designed for evidence estimation (posteriors are a nice byproduct), so the stopping criteria defaults to $\\Delta \\ln \\hat{\\mathcal{Z}}_i < 0.005 (K+1)$ if we intend to \"recycle\" the final set of live points (`add_live=True`; this is the default behavior) and $\\Delta \\ln \\hat{\\mathcal{Z}}_i < 0.01$ otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12573it [00:13, 936.06it/s, +1500 | bound: 6 | nc: 1 | ncall: 68208 | eff(%): 21.096 | loglstar:   -inf < -0.299 <    inf | logz: -9.003 +/-  0.084 | dlogz:  0.001 >  1.509]\n"
     ]
    }
   ],
   "source": [
    "# sample from the target distribution\n",
    "sampler.run_nested()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results can be accessed after a run via `sampler.results` and are stored as a modified dictionary. We can get a quick summary of our results using the `summary()` method, although the actual file contains a host of information about the nested sampling run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['nlive', 'niter', 'ncall', 'eff', 'samples', 'blob', 'samples_id', 'samples_it', 'samples_u', 'logwt', 'logl', 'logvol', 'logz', 'logzerr', 'information', 'bound', 'bound_iter', 'samples_bound', 'scale'] \n",
      "\n",
      "Summary\n",
      "=======\n",
      "nlive: 1500\n",
      "niter: 12573\n",
      "ncall: 66708\n",
      "eff(%): 21.096\n",
      "logz: -9.003 +/-  0.216\n"
     ]
    }
   ],
   "source": [
    "res = sampler.results  # grab our results\n",
    "print('Keys:', res.keys(),'\\n')  # print accessible keys\n",
    "res.summary()  # print a summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most relevant quantities for a general user will be **`samples`** (the collection of samples from the run), **`logwt`** (their corresponding ln-importance weights), **`logz`** (the cumulative ln-evidence), and **`logzerr`** (the error on the ln-evidence). The remaining quantities are used to help visualize the output (see below) and will also be useful for more advanced users who want additional information about the nested sampling run. See the documentation for more detailed information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending a Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can do any better by adding more samples (dead points) until an even more stringent stopping criterion is met. By default, `dynesty` keeps track of the live points and final set of samples, making it easy to add additional samples to the same run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16575it [00:04, 4127.90it/s, +1500 | bound: 9 | nc: 1 | ncall: 75247 | eff(%): 24.509 | loglstar:   -inf < -0.295 <    inf | logz: -9.002 +/-  0.067 | dlogz:  0.000 >  0.100]\n"
     ]
    }
   ],
   "source": [
    "sampler.run_nested(dlogz=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling in Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dynesty` supports evaluating likelihood functions in parallel through a user-provided `pool`. The only requirements are that the pool has a `map` function and supports advanced pickling (via `dill` or `cloudpickle`) to facilitate passing nested function calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using dynesty's own pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most convenient way to sample on a single machine is to use the wrapper around multiprocessing pool provided by dynesty. It is demonstrated below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dynesty.pool as dypool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20024it [00:27, 719.45it/s, +1500 | bound: 26 | nc: 1 | ncall: 509974 | eff(%):  4.233 | loglstar:   -inf < -0.293 <    inf | logz: -9.028 +/-  0.070 | dlogz:  0.000 >  0.010]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with dypool.Pool(4, loglikelihood, prior_transform) as pool:\n",
    "    # The important thing that we provide the loglikelihood/prior transform from \n",
    "    # the pool    \n",
    "    psampler = dynesty.NestedSampler(pool.loglike, pool.prior_transform, ndim, \n",
    "                                 nlive=1500, sample='rslice',pool=pool,\n",
    "                                     rstate=rstate)\n",
    "    psampler.run_nested(dlogz=0.01)\n",
    "pres = psampler.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ipyparallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the excellent **[`ipyparallel`](https://ipyparallel.readthedocs.io/en/latest/)** package to support our parallelization. In this particular example, the underlying cluster is also running the [MPICH](http://www.mpich.org/) implementation of [MPI](https://en.wikipedia.org/wiki/Message_Passing_Interface). See the [`ipyparallel` documentation](https://ipyparallel.readthedocs.io/en/latest/) for additional information on getting set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparallel as ipp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing profile dir: '/Users/lejay/.ipython/profile_default'\n"
     ]
    }
   ],
   "source": [
    "cl = ipp.Cluster(n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have `ipyparallel` imported and a cluster up and running, we need to create a \"client\" to interface with our processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'asyncio' has no attribute 'get_running_loop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9ef56032c671>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_and_connect_sync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnprocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ipyparallel/_async.py\u001b[0m in \u001b[0;36m_synchronize\u001b[0;34m(self, async_f, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \"\"\"\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# not in a running loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'asyncio' has no attribute 'get_running_loop'"
     ]
    }
   ],
   "source": [
    "rc=cl.start_and_connect_sync()\n",
    "nprocs = len(rc.ids)\n",
    "print(rc.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable direct execution, we will then make used of a `DirectView` object and set it to use `dill` instead of the default `pickle`. This helps to avoid some pickling issues that can arise when transferring information to members of the `pool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview = rc[:]\n",
    "dview.use_dill();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although our processors have now been initialized, they do not share global memory. We now need to initialize each member of the group with the same global environment. This can be done using the `%%px` magic function, which automatically runs all commands below it in the same cell in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "# import environment\n",
    "from __future__ import division, print_function\n",
    "from six.moves import range\n",
    "import time, sys, os\n",
    "import numpy as np\n",
    "from numpy import linalg\n",
    "import dynesty\n",
    "\n",
    "# define likelihood constants\n",
    "ndim = 3\n",
    "C = np.identity(ndim)\n",
    "C[C==0] = 0.95\n",
    "Cinv = linalg.inv(C)\n",
    "lnorm = -0.5 * (np.log(2 * np.pi) * ndim + np.log(linalg.det(C)))\n",
    "\n",
    "# seed the random number generator\n",
    "np.random.seed(os.getpid())\n",
    "print('Random seed:',os.getpid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to create an appropriate `pool`. By default, `dview.map` aliases an *asynchronous* map whose results can be accessed afterwards. We, however, require a simpler, *synchronous* `map` that just returns the results of the distributed computation in one batch. This just requires wrapping the `dview.map_sync` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pool(object):\n",
    "    \"\"\"A simple wrapper for `dview`.\"\"\"\n",
    "    \n",
    "    def __init__(self, dview):\n",
    "        self.dview = dview\n",
    "        self.size = nprocs\n",
    "        \n",
    "    def map(self, function, tasks):\n",
    "        return self.dview.map_sync(function, tasks)\n",
    "\n",
    "# define our pool\n",
    "pool = Pool(dview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, our pool is all set! By default, `dynesty` will use our pool to compute most bulk operations in parallel. For illustrative purposes, here we'll opt to do perform ellipsoid decompositions in serial by specifying the appropriate `use_pool` flag. We'll also switch sampling methods to `'rslice'` to illustrate how `dynesty` can sample using slice sampling. Finally, we'll change the bounding/sampling behavior by tweaking the `first_update` argument to illustrate that behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psampler = dynesty.NestedSampler(loglikelihood, prior_transform, ndim, \n",
    "                                 nlive=1500, sample='rslice',\n",
    "                                 first_update={'min_ncall': 5000, 'min_eff': 50.},\n",
    "                                 pool=pool, use_pool={'update_bound': False},\n",
    "                                rstate=rstate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psampler.run_nested(dlogz=0.01)\n",
    "pres = psampler.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running the nested samling for a very long time and on a cluster you may be interested in saving progress during fitting and then resume your fit if it was interrupted.\n",
    "This can be done using checkpoint_file, resume restore options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psampler = dynesty.NestedSampler(loglikelihood, prior_transform, ndim, \n",
    "                                 nlive=500, \n",
    "                                rstate=rstate)\n",
    "psampler.run_nested(checkpoint_file='nested_run.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the run was interrupted and you want to restart your fitting you need to execute the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psampler = dynesty.restore('nested_run.sav')\n",
    "# psampler.run_nested(resume=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Externally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to having our samples generated entirely internally to our `NestedSampler` object via `run_nested()`, `dynesty` can also be run explicitly as a **generator** using the `sample()` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogz_final = 0.01\n",
    "\n",
    "from dynesty.results import print_fn\n",
    "\n",
    "# continue sampling from where we left off\n",
    "ncall = sampler.ncall  # internal calls\n",
    "nit = sampler.it  # internal iteration\n",
    "for it, results in enumerate(sampler.sample(dlogz=dlogz_final)):\n",
    "    # split up our results\n",
    "    (worst, ustar, vstar, loglstar, logvol, logwt, logz, logzvar,\n",
    "     h, nc, worst_it, boundidx, bounditer, eff, delta_logz, blob) = results\n",
    "    # add number of function calls\n",
    "    ncall += nc\n",
    "    nit += 1\n",
    "    # print results\n",
    "    print_fn(results, nit, ncall, dlogz=dlogz_final)\n",
    "\n",
    "# add the remaining live points back into our final results \n",
    "# (they are removed from our set of dead points each time we start sampling)\n",
    "for it2, results in enumerate(sampler.add_live_points()):\n",
    "    # split up results\n",
    "    (worst, ustar, vstar, loglstar, logvol, logwt, logz, logzvar,\n",
    "     h, nc, worst_it, boundidx, bounditer, eff, delta_logz,blob) = results\n",
    "    # print results\n",
    "    print_fn(results, nit, ncall, add_live_it=it2+1, dlogz=dlogz_final)\n",
    "\n",
    "res2 = sampler.results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, note that our estimated evidence $\\ln \\mathcal{Z}_i$ from our original run (`res`) and this extended run (`res2`) are almost identical even though we've almost doubled the total number of samples, although the effective error is somewhat smaller. This demonstrates how much we can gain by \"recycling\" the final set of live points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-defining plotting defaults\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'xtick.major.pad': '7.0'})\n",
    "rcParams.update({'xtick.major.size': '7.5'})\n",
    "rcParams.update({'xtick.major.width': '1.5'})\n",
    "rcParams.update({'xtick.minor.pad': '7.0'})\n",
    "rcParams.update({'xtick.minor.size': '3.5'})\n",
    "rcParams.update({'xtick.minor.width': '1.0'})\n",
    "rcParams.update({'ytick.major.pad': '7.0'})\n",
    "rcParams.update({'ytick.major.size': '7.5'})\n",
    "rcParams.update({'ytick.major.width': '1.5'})\n",
    "rcParams.update({'ytick.minor.pad': '7.0'})\n",
    "rcParams.update({'ytick.minor.size': '3.5'})\n",
    "rcParams.update({'ytick.minor.width': '1.0'})\n",
    "rcParams.update({'font.size': 30})\n",
    "# 3-D plots of position and likelihood, colored by weight\n",
    "fig = plt.figure(figsize=(30, 10))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "\n",
    "# plotting the initial run\n",
    "p = ax.scatter(res.samples[:, 0], res.samples[:, 1], res.samples[:, 2],\n",
    "               marker='o', c=np.exp(res.logwt) * 1e7, linewidths=(0.,), cmap='coolwarm')\n",
    "ax.set_xlim(-10., 10.)\n",
    "ax.set_xticks(np.linspace(-10., 10., 5))\n",
    "ax.set_xlabel(r'$x$', labelpad=25)\n",
    "ax.set_ylim(-10., 10.)\n",
    "ax.set_yticks(np.linspace(-10., 10., 5))\n",
    "ax.set_ylabel(r'$y$', labelpad=25)\n",
    "ax.set_zlim(-10., 10.)\n",
    "ax.set_zticks(np.linspace(-10., 10., 5))\n",
    "ax.set_zlabel(r'$z$', labelpad=25)\n",
    "ax.set_title('Initial Run')\n",
    "cb = fig.colorbar(p)\n",
    "cb.set_label('Weight (1e-6)', labelpad=50., rotation=270.)\n",
    "plt.tight_layout()\n",
    "\n",
    "# plotting the extended run\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "p = ax.scatter(res2.samples[:, 0], res2.samples[:, 1], res2.samples[:, 2],\n",
    "               marker='o', c=np.exp(res2.logwt) * 1e8, linewidths=(0.,), cmap='coolwarm')\n",
    "ax.set_xlim(-10., 10.)\n",
    "ax.set_xticks(np.linspace(-10., 10., 5))\n",
    "ax.set_xlabel(r'$x$', labelpad=25)\n",
    "ax.set_ylim(-10., 10.)\n",
    "ax.set_yticks(np.linspace(-10., 10., 5))\n",
    "ax.set_ylabel(r'$y$', labelpad=25)\n",
    "ax.set_zlim(-10., 10.)\n",
    "ax.set_zticks(np.linspace(-10., 10., 5))\n",
    "ax.set_zlabel(r'$z$', labelpad=25)\n",
    "ax.set_title('Extended Run')\n",
    "cb = fig.colorbar(p)\n",
    "cb.set_label('Weight (1e-8)', labelpad=50., rotation=270.)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a more detailed look at what our nested sampler is doing over the course of our run using several of `dynesty`'s built-in plotting utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynesty import plotting as dyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analytic evidence solution\n",
    "lnz_truth = ndim * -np.log(2 * 10.)  # log(volume) of prior; log(like) is normalized\n",
    "\n",
    "# plot parallel run\n",
    "fig, axes = dyplot.runplot(pres, color='red')\n",
    "\n",
    "# plot extended run\n",
    "fig, axes = dyplot.runplot(res2, color='dodgerblue', fig=(fig, axes))\n",
    "\n",
    "# overplot original run\n",
    "fig, axes = dyplot.runplot(res, color='blue', lnz_truth=lnz_truth, truth_color='black',\n",
    "                           fig=(fig, axes))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that up until we recycle our final set of live points (indicated by the dashed lines), the number of live points is constant as a function of $\\ln X_i$. Afterwards, however, it flattens out, rapidly traversing the remaining prior volume in linear fashion. While this clearly introduces additional sampling noise, the overall effect on $\\ln \\hat{\\mathcal{Z}}$ itself is quite muted (especially given the estimated uncertainties)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traces and Posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the original run\n",
    "fig, axes = dyplot.traceplot(res, truths=[0., 0., 0.], truth_color='black',\n",
    "                             show_titles=True, title_kwargs={'fontsize': 28, 'y': 1.05},\n",
    "                             trace_cmap='plasma', kde=False,\n",
    "                             connect=True, connect_highlight=range(5),\n",
    "                             fig=plt.subplots(3, 2, figsize=(14, 12)))\n",
    "fig.tight_layout()\n",
    "\n",
    "# plotting the continued run\n",
    "fig, axes = dyplot.traceplot(res2, truths=[0., 0., 0.], truth_color='black',\n",
    "                             show_titles=True, title_kwargs={'fontsize': 28, 'y': 1.05},\n",
    "                             trace_cmap='viridis', kde=False,\n",
    "                             connect=True, connect_highlight=range(5),\n",
    "                             fig=plt.subplots(3, 2, figsize=(14, 12)))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that although the evidences we estimate from both runs are almost identical, the (marginalized) posteriors we derive in our extended run is considerably smoother since we have more finely sampled the bulk of the posterior mass (clearly visible in the bottom three left panels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corner Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first take a look at the positions and weights of our individual samples to see where our samples are concentrated using `cornerpoints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize figure\n",
    "fig, axes = plt.subplots(2, 5, figsize=(25, 10))\n",
    "axes = axes.reshape((2, 5))\n",
    "[a.set_frame_on(False) for a in axes[:, 2]]\n",
    "[a.set_xticks([]) for a in axes[:, 2]]\n",
    "[a.set_yticks([]) for a in axes[:, 2]]\n",
    "\n",
    "# plot initial run (left)\n",
    "fg, ax = dyplot.cornerpoints(res, cmap='plasma', truths=[0., 0., 0.],\n",
    "                             kde=False, fig=(fig, axes[:, :2]))\n",
    "\n",
    "# plot extended run (right)\n",
    "fg, ax = dyplot.cornerpoints(res2, cmap='viridis', truths=[0., 0., 0.],\n",
    "                             kde=False, fig=(fig, axes[:, 3:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by looking at our projected samples, it is readily apparent that our extended run does a much better job of localizing the overall distribution. Let's try and get a better qualatative handle on this below by plotting the marginal 1-D and 2-D posteriors using `cornerplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize figure\n",
    "fig, axes = plt.subplots(3, 7, figsize=(35, 15))\n",
    "axes = axes.reshape((3, 7))\n",
    "[a.set_frame_on(False) for a in axes[:, 3]]\n",
    "[a.set_xticks([]) for a in axes[:, 3]]\n",
    "[a.set_yticks([]) for a in axes[:, 3]]\n",
    "\n",
    "# plot initial run (left)\n",
    "fg, ax = dyplot.cornerplot(res, color='blue', truths=[0., 0., 0.], truth_color='black',\n",
    "                           show_titles=True, max_n_ticks=3, title_kwargs={'y': 1.05},\n",
    "                           quantiles=None, fig=(fig, axes[:, :3]))\n",
    "\n",
    "# plot extended run (right)\n",
    "fg, ax = dyplot.cornerplot(res2, color='dodgerblue', truths=[0., 0., 0.], truth_color='black',\n",
    "                           show_titles=True, title_kwargs={'y': 1.05},\n",
    "                           quantiles=None, max_n_ticks=3, fig=(fig, axes[:, 4:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though our original run (with 2x less samples) gave similar evidence estimates, it gives somewhat \"noisier\" estimates of the posterior than those from our extended run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equally weighted samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the samples produced by nested sampling have importance weights, but if you are interested in samples with equal weights that you can work with some other plotting packages like corner or chainconsumer, you can access these through samples_equal() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_equal = res.samples_equal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import corner\n",
    "    corner.corner(samples_equal)\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolving Bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize how we're sampling in nested \"shells\", we can look at the evolution of our bounding distributions in a given 2-D projection over the course of the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize figure\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# plot 6 snapshots over the course of the run\n",
    "for i, a in enumerate(axes.flatten()):\n",
    "    it = int((i+1)*res2.niter/8.)\n",
    "    # overplot the result onto each subplot\n",
    "    temp = dyplot.boundplot(res2, dims=(0, 1), it=it, prior_transform=prior_transform, max_n_ticks=3,\n",
    "                            show_live=True, span=[(-10, 10), (-10, 10)], fig=(fig, a))\n",
    "    a.set_title('Iteration {0}'.format(it), fontsize=26)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternately, `cornerbound` generates corner plots for our proposals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize figure\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "# plot proposals in corner format\n",
    "fg, ax = dyplot.cornerbound(res2, it=7000, prior_transform=prior_transform, show_live=True, \n",
    "                            span=[(-10, 10), (-10, 10)], fig=(fig, axes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the default multi-ellipsoid proposals are able to adapt well to the target distribution over time, ensuring we continue to make efficient proposals without violating our uniform sampling assumptions. We can also see the impact our bootstrapping procedure has on the bounding ellipsoids: they always remain slightly larger than the set of live points. While this slightly decreases our efficiency, it helps ensure we do not \"leave out\" any likelihood during the course of the run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested sampling is a powerful and versatile approach for computing Bayesian evidences (with posterior estimates as a \"free\" by-product). Various nested sampling algorithms can be implemented using the `NestedSampler` \"class\" in `dynesty`, which allow for a variety of bounding and sampling methods. Sampling can be done implicitly (within `NestedSampler`) or explicitly (as a generator) depending on the degree of control users want over the outputs. Sampling can also be done in parallel using a user-supplied `pool`. Results can be accessed via the `results` property and can be visualized using several built-in plotting functions (via `plotting`)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
